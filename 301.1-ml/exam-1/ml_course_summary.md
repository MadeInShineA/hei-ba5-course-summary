# Machine Learning Course Summary\n\n## Table of Contents\n1. [Introduction to Machine Learning](#introduction-to-machine-learning)\n2. [Mathematical Foundations](#mathematical-foundations)\n3. [Model Evaluation](#model-evaluation)\n4. [Laboratory Projects](#laboratory-projects)\n5. [Key Takeaways](#key-takeaways)\n\n---\n\n## Introduction to Machine Learning\n\n### What is Machine Learning?\n\nMachine Learning is a field of study that gives computers the ability to learn without being explicitly programmed (A. Samuel). At its core:\n\n- **Learning** means getting better at some given task\n- **Machine learning** is about writing computer programs that get better at given tasks by using data\n\n### Applications of ML\nML is everywhere in modern applications:\n- Self-driving cars\n- LLMs (Large Language Models)\n- Intrusion detection\n- Stock market prediction\n\n### A Brief History of ML\n\n#### Mathematical Pre-history\n- 1763: Bayes theorem (T. Bayes)\n- 1805: Least squares fitting (A. M. Legendre, G. F. Gauss)\n\n#### Early Days\n- 1943: Mathematical model of artificial neuron (Pitts and Mc Culloch)\n- 1951: First implementation of a neural network (M. Minsky)\n- 1952: First \"intelligent\" program (A. Samuel)\n- 1957: The Perceptron (F. Rosenblatt)\n- 1974-1980: First AI winter\n\n#### The AI Spring\n- 1982-1985: Backpropagation algorithm (S. Linnainmaa, P. Werbos, D. Rumelhart, G. Hinton, R. Williams)\n- 1989: Start of modern RL (C. Watkins)\n- 1995: Random Forest (T. K. Ho)\n- 1997: Deep Blue (IBM)\n\n#### Modern Boom\n- 2011-2012: AlexNet (A. Krizhevsky, I. Sutskever, G. Hinton), ML becomes good at vision\n- 2016: AlphaGo (DeepMind), ML beats humans at complex games\n- 2017: Transformer Architecture\n- 2018: AlphaFold, ML performs a breakthrough in natural science\n- 2022: ChatGPT, ML becomes good at natural language\n\n### The ML Lifecycle\n\n1. **Collect some data**\n2. **Write a program that \"learns\" from it (training)**\n3. **Use the program to \"predict\" what happens for new, unseen data**\n\n### Types of Machine Learning\n\n- **Supervised Learning**: Learning from labeled data\n- **Unsupervised Learning**: Learning from unlabeled data\n- **Reinforcement Learning**: Learning from interaction with an environment\n\n### Supervised Learning\n\nIn supervised learning, we have labeled data consisting of training examples:\n$$ \\mathcal{D} = \\{ (\\mathbf{x}^{(i)}, y^{(i)}) | i = 1, \\ldots, n \\} $$\n\nOur goal is to find a \"model\" $f$ that is able to predict $y$ for unseen values of $\\mathbf{x}$.\n\n**Notation:**\n- Boldface is used to denote vectors\n- $\\mathbf{x}$ denotes a single training example, the associated response being denoted by $y$\n- Individual components $\\mathbf{x}_j$ denote single features of a given example\n- Superscript denote different training examples: $\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\ldots$\n\n### First Supervised Learning Algorithm: Linear Regression\n\nA linear model is a model which, given some data point $x$ predicts the response $y$ using a linear function:\n$$ \\hat{y} = f(x) = a \\cdot x + b $$\n\n---\n\n## Mathematical Foundations\n\n### The Core Components of ML\n\n**ML = Model space + Loss function (+ regularization) + optimization**\n\n### Supervised Learning Setup\n\nWe have a (labeled) dataset of input-output examples:\n$$ \\mathcal{D} = \\{(\\mathbf{x}^{(i)}, y^{(i)}) | i = 1, \\ldots, n\\} $$\n\nAnd we want to find a model $f$ that, given an unseen data pair $(x, y)$, performs well at predicting $y$:\n$$ f(x) \\approx y $$\n\n### Model Space and Parameters\n\nTo solve the problem of finding the best model and make the model search tractable, we restrict our search to a given model space $\\mathcal{M}$. The learning task is then to find the best $f$ in $\\mathcal{M}$.\n\nIn most cases, our models are parametrized by a finite set of parameters:\n$$ \\boldsymbol{\\theta} = (\\theta_1, \\ldots, \\theta_p) \\in \\Theta $$\nthat belong to a parameter space $\\Theta$.\n\n### Examples of Model Spaces\n\n#### Linear Regression Models\n- **1D Linear regression model**: $\\mathcal{M} = \\{f(x) = a \\cdot x + b | a, b \\in \\mathbb{R}\\}$\n- **Parameters**: $(a, b) =: \\boldsymbol{\\theta} \\in \\Theta := \\mathbb{R}^2$\n- **For a given parameter $\\boldsymbol{\\theta} = (a, b)$**: $f_{\\boldsymbol{\\theta}}(x) = a \\cdot x + b$\n\n#### Polynomial Regression Models\n- **1D Polynomial Regression Model**: Model space is the set of all polynomial functions of $x$, up to degree $d$:\n  $$ \\mathcal{M} = \\{f(x) = a_1 \\cdot x + a_2 \\cdot x^2 + \\ldots + a_d \\cdot x^d + b | a_1, \\ldots, a_d, b \\in \\mathbb{R}\\}$\n- **Parameters**: $(b, a_1, \\ldots, a_d) =: \\boldsymbol{\\theta} \\in \\Theta := \\mathbb{R}^{d+1}$\n\n### The Bias-Variance Trade-off\n\n- **Simple models** (small $\\mathcal{M}$): High bias (has difficulty fitting the data), low variance (low sensitivity to noise)\n- **Complex models** (large $\\mathcal{M}$): Low bias (can fit any data), high variance (too sensitive to data noise)\n\n### Loss Functions\n\nThe idea is to quantify how \"wrong\" our predictions are. For each example $(\\mathbf{x}, y)$ in the training dataset, we want to know how good our prediction $\\hat{y} = f(x)$ is.\n\n#### L2 Loss (Mean Squared Error)\n$$ L(y, \\hat{y}) = (y - \\hat{y})^2 $$\n- Penalizes large errors more than small ones\n- Influenced by outliers\n- Most common loss function for regression\n\n#### L1 Loss (Mean Absolute Error)\n$$ L(y, \\hat{y}) = |y - \\hat{y}| $$\n- More robust to outliers\n- Less statistically well-behaved\n\n### Empirical Risk Minimization\n\nThe central idea is to sum all losses $L(y^{(i)}, \\hat{y}^{(i)})$ over the training dataset for each possible value of the parameter $\\boldsymbol{\\theta}$:\n\n$$ J(\\boldsymbol{\\theta}) = \\sum_{i=1}^{n} L(y^{(i)}, f_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)})) $$\n\nThis tells us how well the model $f_{\\boldsymbol{\\theta}}$ fits the data for a given $\\boldsymbol{\\theta}$.\n\nThe best model is the one with the smallest loss (smallest risk):\n$$ \\boldsymbol{\\theta}^* = \\arg\\min_{\\boldsymbol{\\theta} \\in \\Theta} J(\\boldsymbol{\\theta}) $$\n\n### Optimization Methods\n\n- Some models have analytical solutions (e.g., linear regression)\n- In general, perform iterative minimization using methods like gradient descent\n- Scipy provides optimization routines for general cases\n\n---\n\n## Model Evaluation\n\n### Train-Validation-Test Split\n\nThe best practice in ML is to split the data into:\n- **Training data** (60%): Used for model training\n- **Validation data** (20%): Used for model selection\n- **Test data** (20%): Used for final, honest performance evaluation\n\n**Key principle**: Test data is never touched during model training and selection!\n\n### Overfitting and Underfitting\n\n#### Overfitting\n- Model too complex\n- Low training error\n- High validation error\n- Solution: Regularization, more data, simpler model\n\n#### Underfitting\n- Model too simple\n- High training error\n- High test error\n- Solution: More complex model, more features\n\n### Model Selection Process\n1. Design different models\n2. Train all models on the train set\n3. Select best model by comparing performances on validation set\n\n### Bias-Variance Tradeoff\n\n- **Bias**: Error from wrong assumptions (underfitting)\n- **Variance**: Error from sensitivity to training data (overfitting)\n- **Sweet spot**: Minimize total error\n\n### The No Free Lunch Theorem\n\n\"There is no such thing as a free lunch.\" (D. Wolpert and W. MacReady)\n\nAll models are wrong but some are useful. (G. Box)\n\n### Regularization\n\nRegularization adds a penalty term to the loss function to constrain model complexity:\n\n$$ J_{\\text{regularized}}(\\boldsymbol{\\theta}) = J_{\\text{original}}(\\boldsymbol{\\theta}) + \\lambda \\cdot R(\\boldsymbol{\\theta}) $$\n\nWhere:\n- $J_{\\text{original}}(\\boldsymbol{\\theta})$: Original loss (e.g., MSE)\n- $\\lambda$: Regularization strength (hyperparameter)\n- $R(\\boldsymbol{\\theta})$: Penalty term on parameters\n\n#### L2 Regularization (Ridge Regression)\n$$ J_{\\text{Ridge}}(\\boldsymbol{\\theta}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} \\theta_j^2 $$\n- Penalizes squared magnitude of coefficients\n- Shrinks coefficients toward zero (but not exactly zero)\n\n#### L1 Regularization (Lasso)\n$$ J_{\\text{Lasso}}(\\boldsymbol{\\theta}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} |\\theta_j| $$\n- Penalizes absolute magnitude of coefficients\n- Can shrink coefficients to exactly zero\n- Performs a kind of automatic feature selection\n\n### Residuals Analysis\n\nResiduals are the part of the data that is not captured by the model:\n$$ r_i := y_i - \\hat{y}_i $$\n\nIn an ideal world, residuals should be i.i.d. Gaussian distributed.\n\n**What to check in residuals:**\n- Plot residuals vs target variable - is there structure?\n- Is the variance of the residuals constant (homoscedasticity)?\n- Are the residuals normally distributed (qq-plot)?\n\n```mermaid\ngraph TD\n    A[Collect Data] --> B[Train-Validation-Test Split]\n    B --> C[Model Training on Train Set]\n    C --> D[Model Validation on Validation Set]\n    D --> E[Model Selection]\n    E --> F[Final Evaluation on Test Set]\n    F --> G[Analyze Residuals]\n    G --> H[Iterate/Improve Model]\n    H --> C\n```\n\n---\n\n## Laboratory Projects\n\n### Mini Lab 1: Fuel Consumption Analysis\n\nThis project focused on predicting CO2 emissions from car fuel consumption data.\n\n#### Key Findings from the Dataset\n- **Dataset**: Contains 17,766 car models with features like:\n  - Model year\n  - Engine size (L)\n  - Cylinders\n  - City/Highway fuel consumption (L/100 km)\n  - Combined fuel consumption (L/100 km)\n  - CO2 emissions (g/km)\n  - Fuel type (X, Z, D, N, E)\n\n#### Correlation Analysis\nStrong correlations observed between:\n- Engine size and cylinders (0.9018)\n- Engine size and fuel consumption (0.8224 for city)\n- City and highway fuel consumption (0.9531)\n- Fuel consumption and CO2 emissions (0.9260)\n\n#### Simple Linear Model\nThe model used was:\n$$ \\text{CO2 emissions} = a \\cdot \\text{Combined (L/100 km)} $$\n\nBest parameter found: $a = 22.3372$\n\n#### Extended Model with Fuel Type\nTo improve the model, fuel type was incorporated:\n$$ \\text{CO2 emissions} = a \\cdot \\text{Combined (L/100 km)} + b \\cdot \\text{is_diesel} + c \\cdot \\text{is_essence} $$\n\nBest parameters found:\n- $a = 22.959$\n- $b = 35.373$ (for diesel)\n- $c = -117.457$ (for essence)\n\n#### Model Performance\n- Simple linear model: $R^2 = 0.637$\n- Extended model: $R^2 = 0.837$ (significant improvement)\n\n### Project 1: House Price Analysis\n\nThis project focused on predicting house prices from various features.\n\n#### Key Findings from the Dataset\n- **Dataset**: Contains 1,364 house entries with features like:\n  - Living area (surf_hab)\n  - Building material quality (qualite_materiau)\n  - Basement area (surface_sous_sol)\n  - Overall quality (qualite_globale)\n  - Garage capacity (n_garage_voitures)\n  - Number of toilets (n_toilettes)\n  - Number of fireplaces (n_cheminees)\n  - Number of rooms (n_pieces)\n  - Number of kitchens (n_cuisines)\n  - Number of bedrooms (n_chambres_coucher)\n  - Sale year (annee_vente)\n  - Roof type (type_toit)\n  - Building type (type_batiment)\n  - Kitchen quality (qualite_cuisine)\n\n#### Feature Engineering\n- Created new feature: `surface_by_room = surf_hab / n_pieces`\n- Categorical variables were encoded numerically:\n  - Kitchen quality: {mediocre: 0, moyenne: 1, bonne: 2, excellente: 3}\n  - Building type: {individuelle: 0, individuelle reconvertie: 1, duplex: 2, bout de rangée: 3, milieu de rangée: 4}\n\n#### Model Selection Process\n- **Training dataset**: 60% of data\n- **Validation dataset**: 20% of data\n- **Test dataset**: 20% of data\n- Outliers were removed from training and validation sets (houses with price > 600000)\n\n#### Linear Regression Model\nSelected features for the model:\n- Surface per room (surface_by_room)\n- Building material quality (qualite_materiau)\n- Living area (surf_hab)\n\nModel coefficients:\n- surface_by_room: 50.8421\n- qualite_materiau: 32,665.9383\n- surf_hab: 48.7393\n\n#### Model Performance\n- **Training set**: $R^2 = 0.7255$\n- **Validation set**: $R^2 = 0.6710$\n- **Test set**: $R^2 = 0.7944$\n\n#### Residual Analysis\nThe residual plots showed that while the model captures most of the variance in the data, there are some systematic patterns in the residuals, suggesting room for improvement through additional features or a more complex model.\n\n```mermaid\ngraph TD\n    A[House Data] --> B[Data Preprocessing]\n    B --> C[Feature Engineering]\n    C --> D[Train-Val-Test Split]\n    D --> E[Linear Regression Model]\n    E --> F[Performance Evaluation]\n    F --> G[Residual Analysis]\n    G --> H[Model Refinement]\n```\n\n---\n\n## Key Takeaways\n\n### 1. ML Workflow\n1. **Data Collection**: Gather relevant and representative data\n2. **Data Preprocessing**: Clean, encode, and explore data\n3. **Feature Engineering**: Create meaningful features from raw data\n4. **Model Selection**: Choose appropriate model family\n5. **Training**: Fit the model to training data\n6. **Validation**: Select best model using validation data\n7. **Testing**: Evaluate final performance on test data\n8. **Analysis**: Interpret results and diagnose model behavior\n\n### 2. Critical Concepts\n- **Supervised Learning**: Learning from labeled examples\n- **Overfitting**: When model learns training data too well, including noise\n- **Underfitting**: When model is too simple to capture underlying patterns\n- **Bias-Variance Tradeoff**: Balance between model simplicity and complexity\n- **Cross-validation**: Robust method for model evaluation\n\n### 3. Evaluation Metrics\n- **R² (Coefficient of Determination)**: Proportion of variance explained\n- **RMSE (Root Mean Squared Error)**: Average prediction error in target units\n- **MAE (Mean Absolute Error)**: Average absolute prediction error\n- **Residual Analysis**: Essential for understanding model performance\n\n### 4. Best Practices\n- Always split data before any preprocessing to avoid leakage\n- Use separate train/validation/test splits for different purposes\n- Consider feature engineering to create more meaningful inputs\n- Regularize models to prevent overfitting\n- Analyze residuals to understand model limitations\n- Iterate based on empirical results\n\n### 5. Linear Regression Insights\n- Simple but powerful baseline for regression problems\n- Sensitive to outliers (consider robust alternatives like L1 loss)\n- Coefficients provide interpretable relationships between features and target\n- Works best when relationships are approximately linear\n\n### 6. Feature Importance\n- Domain knowledge can guide feature selection\n- Correlation analysis helps identify important variables\n- Feature engineering can create more predictive inputs\n- Regularization can perform automatic feature selection\n\n### 7. Model Complexity\n- Start with simple models as baselines\n- Increase complexity only if justified by performance improvement\n- Balance model interpretability with performance\n- Be aware of the curse of dimensionality\n\nThis comprehensive approach to machine learning ensures systematic model development with proper evaluation and validation, leading to more reliable and generalizable results.\n
